{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h82o5fIIOQ53"
   },
   "source": [
    "# Introduction to Dask\n",
    "by Dr Stef Garasto\n",
    "\n",
    "<p>\n",
    "Dask is a popular python library for parallel computing. It is designed to integrate easily with existing python code and other common libraries like Numpy and Pandas. During this introductory tutorial you will get to know some fundamental Dask functions, like dask.delayed and dask.dataframe, and workflows. There will also be some hands-on coding exercises for you to practice your newfound Dask knowledge. \n",
    "</p>\n",
    "\n",
    "### References (and sources of inspiration):\n",
    "[1] Dask Tutorial by https://github.com/dask/dask-tutorial/\n",
    "\n",
    "[2] Dask documentation https://docs.dask.org/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYOpvjnuSQJa"
   },
   "source": [
    "Technical reminder: press shift-enter to execute single cells in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzXmQA-8bhAw"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHPSe3jgOKhZ"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "LOCAL = True\n",
    "if not LOCAL:\n",
    "    # Install some requirements\n",
    "    !pip install snakeviz\n",
    "    %pip install \"tornado>=5\" \n",
    "    !pip -q install dask\n",
    "    !pip -q install distributed\n",
    "    #!pip -q install --upgrade --ignore-installed numpy pandas scipy sklearn\n",
    "    !pip -q install graphviz \n",
    "    !apt-get install graphviz -qq\n",
    "    !pip -q install pydot\n",
    "    !pip -q install bokeh\n",
    "    !pip -q install 'fsspec>=0.3.3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NqVE9F1ML43"
   },
   "outputs": [],
   "source": [
    "if not LOCAL:\n",
    "    # to get the data\n",
    "    !rm -r dask-mini-tutorial\n",
    "    !git clone https://github.com/stefgrs/dask-mini-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svi7ZPXicAYR"
   },
   "outputs": [],
   "source": [
    "# collect the dataset\n",
    "%run prep.py -d flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ki9CvItHQZIl"
   },
   "outputs": [],
   "source": [
    "# setup the Dask scheduler (just go with it for now!)\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "client = Client(n_workers=4, processes = False)\n",
    "client\n",
    "\n",
    "#import dask\n",
    "#dask.config.set(scheduler='threads')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrMqvx-ZUinS"
   },
   "source": [
    "# Pandas dataframes\n",
    "\n",
    "Pandas is a python library for data manipulation and analysis. A dataframe is the main data structure in pandas. In the vast majority of cases a pandas dataframe is created by reading a '.csv' file from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ExDI2mHEEwSt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# let's have a look at a pandas DataFrame\n",
    "# load sample data\n",
    "titanic = pd.read_csv('data/titanic.csv')\n",
    "\n",
    "# visualise the dataframe\n",
    "titanic.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GWVSMSMTKurL"
   },
   "outputs": [],
   "source": [
    "titanic.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4sgNahdYDDx"
   },
   "outputs": [],
   "source": [
    "# each row is uniquely identified by its index\n",
    "titanic.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YA_IFnrE3dc"
   },
   "outputs": [],
   "source": [
    "# pandas lets us easily compute summary statistics on a dataframe ... \n",
    "titanic.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1PRzxtzHPvy"
   },
   "outputs": [],
   "source": [
    "# ... or the full distribution for a variable\n",
    "# this counts how many instances there are for each unique value in a column\n",
    "titanic['Pclass'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rTqyWmSGHXFI"
   },
   "outputs": [],
   "source": [
    "# Plots are nicely handled as well\n",
    "titanic['Age'].plot(kind= 'hist')\n",
    "_ = plt.xlabel('Age')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUSSJzGgbT8z"
   },
   "outputs": [],
   "source": [
    "# How much memory does a dataframe uses?\n",
    "titanic.info()\n",
    "print('On disk, the memory usage is 59 KB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T4k73eyrfQUF"
   },
   "outputs": [],
   "source": [
    "# let's try another one\n",
    "df_pandas = pd.read_csv(os.path.join('data','nycflights','1990.csv'))\n",
    "\n",
    "df_pandas.info()\n",
    "print('On disk, the memory usage is around 22 MB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Irg4bRPRfZ7d"
   },
   "source": [
    "The RAM footprint of a pandas dataframe can be higher than the space it occupies on disk! So always check in python (rather than within file explorer) to make sure the dataset will fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bMkDy3FHp55"
   },
   "source": [
    "# Dask dataframes\n",
    "\n",
    "At times, we might have lots of data (think Terabytes). We already saw how `dask.delayed` can be applied to custom algorithms. When we are operating on dataframe-like data, though, it is more convenient to use dask's out-of-the-box tool `dask.dataframe` instead. It is particularly useful to work with data that doesn't fit in the RAM (although we won't work with data this big today)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8r0o2rNXbiV"
   },
   "source": [
    "A Dask dataframe is basically a collection of pandas dataframe. These dataframes are called partitions, or chunks. The split happens along the index, not along the columns.  A single method call on a Dask DataFrame consists of multiple calls on pandas DataFrame, which then are intelligently combined together by Dask to obtain the final result.\n",
    "\n",
    "Since the parallelism happens by partition, this means that only one partition at a time has to fully fit into the RAM. Once Dask is done processing a single partition, it can release the data from memory and load the next partition. This is why Dask allows us to work with datasets that are much larger than memory, as long as each partition (a regular pandas DataFrame) fits in memory.\n",
    "\n",
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\" width=\"40%\">\n",
    "\n",
    "Source image: Dask tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkw8b-TJYRZ8"
   },
   "source": [
    "## Creating a dask dataframe\n",
    "\n",
    "In theory, we can create a dask dataframe from a pandas dataframe. But if we can load the full dataframe with pandas, we don't really need to use Dask!\n",
    "\n",
    "There are two main way of creating a dask dataframe (both of them use the function `dask.dataframe.read_csv``, only with slightly different arguments. One way consists of creating one dask dataframe from multiple files on disk, the other way consists of reading one large file on disk in a 'chunk-by-chunk' way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKwcicRdY5ZV"
   },
   "source": [
    "### Dask reading multiple files\n",
    "\n",
    "At times, it can be that the pandas dataframe is already organised into multiple files (e.g. by year): each of these files fit into memory, but taken together they are too large. In this case we can create a dask dataframe where each partition corresponds to one of the files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0vItbX4SGkv"
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd \n",
    "import os\n",
    "filenames= os.path.join('data','nycflights','*.csv')\n",
    "#*.csv means collect all files whose name follow that pattern. \n",
    "# Look into the folder data/nycflights: can you spot the pattern?\n",
    "df = dd.read_csv(filenames, parse_dates={'Date': [0, 1, 2]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ixbkj4wIhwxe"
   },
   "source": [
    "What just happened?\n",
    "\n",
    "*    Dask investigated the input path and found that there are 10 matching files\n",
    "*    Dask created a chunk (or partition) for each file\n",
    "\n",
    "But what's the object it created?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrxm1DC8YQ7y"
   },
   "outputs": [],
   "source": [
    "# it's kind of empty, really! Because dask it's lazy: it knows where the data is and it's\n",
    "# read it to load it, but it won't actually load it until it absolutely has to!\n",
    "# However, note that we have information about how many partitions Dask created. \n",
    "# In this case it's 10, exactly equal to the number of files that have been read.\n",
    "df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rviiYHlviel2"
   },
   "outputs": [],
   "source": [
    "# for example, Dask will load the first few rows if we call:\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvumisCEjHOQ"
   },
   "outputs": [],
   "source": [
    "# can we compute how many rows are there in the dataframe?\n",
    "len(df) # this fails\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vkm1xBu4jOVm"
   },
   "source": [
    "Why did it fail?\n",
    "\n",
    "The error is as explicit as it can be. It says 'ValueError: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.'\n",
    "\n",
    "But what does it mean? Because of dask's laziness, sometimes it makes certain assumptions that don't hold up to further scrutiny... Specifically, dask will infer the datatypes for each column only from a sample taken from the beginning of the file. It then applies the same datatypes to the rest of the dataframe. But sometimes things change! Imagine for example if a certain piece of data is only collected from 1993 onwards...\n",
    "\n",
    "When this happens, there is a 'mismatched datatype' and Dask breaks. To make sure this doesn't happen, the ideal solution is to specify the data types directly using the dtype keyword. Always check the schema for your data! What would you expect to have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Jzp8WHwjNwO"
   },
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})\n",
    "\n",
    "# all should be good now!\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOAJlwdQk_2Y"
   },
   "source": [
    "### Dask reading one large file in chunks\n",
    "\n",
    "What if we have one big file to load?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoLQ-XUzc15i"
   },
   "outputs": [],
   "source": [
    "# We can load one big file in chunks as well\n",
    "df_from_large = dd.read_csv(os.path.join('data', 'nycflights', '1990.csv'),\n",
    "                 blocksize = '5MB', #we can specify how large we want each block to be\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})\n",
    "\n",
    "# based on blocksize, dask will decide how many partitions to create. \n",
    "# how many are there?\n",
    "print(f'Number of partitions created: {df_from_large.npartitions}')\n",
    "print()\n",
    "\n",
    "# show the (lazy) dataframe\n",
    "df_from_large\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNAKIhti5eHp"
   },
   "source": [
    "#### Exercise: create dask dataframe partitions from one large data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Ef2ckxcveRZ"
   },
   "outputs": [],
   "source": [
    "# Exercise\n",
    "# How can we create a dask dataframe where the size of each partition is 10MB?\n",
    "# How many partitions do we get?\n",
    "######################\n",
    "### Your code here ###\n",
    "######################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpF6CQnemzxh"
   },
   "source": [
    "## Using dask dataframes\n",
    "\n",
    "Using dask dataframe is generally very similar to using pandas dataframe! The same functions apply, with few caveats:\n",
    "\n",
    "- We have to call 'compute' to get the final results, since that's what prompts the task graph to actually execute. Note that execution includes loading the data from scratch every time.\n",
    "- Not all pandas functions are available (yet!)\n",
    "- Some operations are more expensive than in pandas, with one notable example being 'setting the index'. This is because setting the index involves reshuffling of rows, which means that all partitions have to talk to each other. However, it's much easier for communication to happen WITHIN a partition that BETWEEN partitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrnXjv2C5U3e"
   },
   "source": [
    "### Exercise: query the dask dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mvvZQqho4IQ"
   },
   "outputs": [],
   "source": [
    "# For example, how would you check whether there is an equal distribution of flight leaving\n",
    "# on any given day of the week? That is, how many flights have left on different days of \n",
    "# the week? \n",
    "# (hint: there is a column called 'DayOfWeek'. Also, remember the method '.value_counts'?)\n",
    "######################\n",
    "### Your code here ###\n",
    "######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q99MH-t6pr_u"
   },
   "outputs": [],
   "source": [
    "# What if we wanted to know how many flights got cancelled?\n",
    "# hint: there is a column called 'Cancelled' that is a boolean value\n",
    "######################\n",
    "### Your code here ###\n",
    "######################\n",
    "\n",
    "# yep, same function as pandas + compute!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdXXKPl6qhNz"
   },
   "outputs": [],
   "source": [
    "# Just out of curiosity, let's see what the computation graph looks like...\n",
    "df.Cancelled.sum().visualize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMOIY880q8O4"
   },
   "outputs": [],
   "source": [
    "df.DayOfWeek.value_counts().visualize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5uc37jpqNOL"
   },
   "source": [
    "## Dask-specific functions\n",
    "\n",
    "We can create our own custom operations on a Dask dataframe. One useful function to know is [map_partitions](https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions).\n",
    "\n",
    "One can use `map_partitions` to apply a custom function on each partition. The output is the combined results from all partitions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trHfLMS2nuV7"
   },
   "outputs": [],
   "source": [
    "# Let's say we want to know how many morning flights there are among those that didn't get cancelled \n",
    "def get_morning_flights_nb(df):\n",
    "    # get non cancelled flights\n",
    "    df_subset = df[~df.Cancelled]\n",
    "    # returns which flight left before 12:00pm\n",
    "    return (df_subset.DepTime<1200)\n",
    "\n",
    "df.map_partitions(get_morning_flights_nb).sum().compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvVWy0OD5Ni0"
   },
   "source": [
    "### Exercise: apply 'map_partitions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zu-RuUv4xlW"
   },
   "outputs": [],
   "source": [
    "# What if we wanted to know how many morning flights there are among those that didn't get cancelled \n",
    "# and that left from JFK?\n",
    "# hint: check out the possible values in the column 'Origin' (JFK, EWR, LGA)\n",
    "df.Origin.value_counts().compute()\n",
    "\n",
    "######################\n",
    "### Your code here ###\n",
    "######################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZk_fj5ryVpi"
   },
   "source": [
    "## Questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fr8JGGV621MW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Dask-tutorial-Mar2021-dataframe.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
